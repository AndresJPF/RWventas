{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e60d8d3",
   "metadata": {},
   "source": [
    "# Cleaning and Conections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc70c84",
   "metadata": {},
   "source": [
    "#### 1. Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9c5bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   Ciudad           995449 non-null  object\n",
      " 1   Fecha            995380 non-null  object\n",
      " 2   Producto         995395 non-null  object\n",
      " 3   Tipo_Producto    995374 non-null  object\n",
      " 4   Cantidad         995470 non-null  object\n",
      " 5   Precio_Unitario  995545 non-null  object\n",
      " 6   Tipo_Venta       995477 non-null  object\n",
      " 7   Tipo_Cliente     995579 non-null  object\n",
      " 8   Descuento        995497 non-null  object\n",
      " 9   Costo_Envio      995531 non-null  object\n",
      " 10  Total            995472 non-null  object\n",
      "dtypes: object(11)\n",
      "memory usage: 83.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "df = pd.read_csv(\"RWventas.csv\")\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b42ea",
   "metadata": {},
   "source": [
    "1.1 Visialization of corrupted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2582213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ciudad' ['Antofagasta' 'Monterrey' 'Valparaíso' 'Sevilla' 'Córdoba'\n",
      " 'Ciudad de México' 'Houston' 'Mendoza' 'Barcelona' 'Chicago' 'Miami'\n",
      " 'Pereira' 'Cali' 'Concepción' 'Rosario' 'Madrid' 'Arequipa' 'Lima'\n",
      " 'Santiago' 'Bucaramanga' 'Tijuana' 'Guadalajara' 'Cusco' 'New York'\n",
      " 'Buenos Aires' 'Valencia' 'Los Angeles' 'Bogotá' 'Trujillo' nan 'Puebla'\n",
      " 'Medellín' 'Barranquilla' 'Cartagena' 'Barcelona***' 'Córdoba***'\n",
      " 'Ciudad de México###' 'Barcelona@@@' 'Madrid###' 'Pereira@@@'\n",
      " 'Santiago***' '  Barcelona   ' 'Miami###' 'vALPARAÍSO' 'cÓRDOBA'\n",
      " '  Miami   ' 'Puebla###' 'Medellín@@@' 'Sevilla***' 'Puebla***'\n",
      " '  Valparaíso   ' '  Bogotá   ' 'Bogotá###' '  Ciudad de México   '\n",
      " '  Pereira   ' 'cUSCO' 'Cali***' 'Barcelona###' 'pEREIRA' '  Cali   '\n",
      " 'Buenos Aires***' 'cONCEPCIÓN' 'nEW yORK' '  Trujillo   ' '  Sevilla   '\n",
      " '  Antofagasta   ' 'Mendoza***' 'gUADALAJARA' 'Trujillo@@@' 'hOUSTON'\n",
      " '  Concepción   ' 'Cusco###' 'Madrid@@@' 'Antofagasta###' 'mEDELLÍN'\n",
      " 'tIJUANA' '  Guadalajara   ' '  Madrid   ' '  Cartagena   ' 'Sevilla@@@'\n",
      " '  Mendoza   ' 'lIMA' 'cALI' 'Chicago***' 'Tijuana@@@' 'bUCARAMANGA'\n",
      " 'Valencia***' 'Houston###' 'mONTERREY' 'aNTOFAGASTA' 'Miami@@@'\n",
      " 'Mendoza@@@' 'Rosario###' 'bUENOS aIRES' 'mIAMI' 'Cartagena@@@'\n",
      " 'cIUDAD DE mÉXICO' 'Córdoba###' 'Arequipa###' 'mENDOZA' '  Tijuana   '\n",
      " '  Barranquilla   ' 'Lima***' 'New York###' 'bARCELONA' 'Los Angeles@@@'\n",
      " '  Monterrey   ' '  Medellín   ' 'Rosario***' 'aREQUIPA' '  Puebla   '\n",
      " 'Concepción***' 'sEVILLA' 'lOS aNGELES' '  Chicago   ' 'pUEBLA' 'bOGOTÁ'\n",
      " '  Rosario   ' '  Santiago   ' 'vALENCIA' 'Cartagena***' 'Cali@@@'\n",
      " 'Houston***' 'Monterrey@@@' 'Tijuana***' 'Córdoba@@@' 'Lima@@@'\n",
      " 'Trujillo***' 'Valparaíso***' 'sANTIAGO' 'Valparaíso###' 'Houston@@@'\n",
      " '  Lima   ' '  Valencia   ' 'tRUJILLO' '  Buenos Aires   '\n",
      " 'Bucaramanga@@@' 'Monterrey###' 'Antofagasta@@@' 'Ciudad de México@@@'\n",
      " 'Buenos Aires###' 'Tijuana###' 'mADRID' 'Trujillo###' 'Medellín***'\n",
      " 'Cusco***' 'Lima###' 'cARTAGENA' 'rOSARIO' 'Miami***' 'Barranquilla###'\n",
      " 'Concepción###' 'cHICAGO' 'bARRANQUILLA' 'Los Angeles###' 'Cali###'\n",
      " '  Cusco   ' 'Arequipa@@@' 'Valencia###' '  Houston   ' 'Guadalajara***'\n",
      " 'Pereira###' 'Bogotá***' '  New York   ' '  Bucaramanga   '\n",
      " '    Concepción      ' 'Bucaramanga###' '  Arequipa   ' 'Buenos Aires@@@'\n",
      " 'Barranquilla@@@' 'Cusco@@@' 'Los Angeles***' 'Mendoza###'\n",
      " 'Concepción@@@' 'Santiago###' 'Rosario@@@' 'Antofagasta***' 'Madrid***'\n",
      " '  Los Angeles   ' 'Monterrey***' 'Puebla@@@' 'Arequipa***'\n",
      " '  Córdoba   ' 'New York@@@' 'New York***' 'Pereira***' 'Valparaíso@@@'\n",
      " 'Guadalajara@@@' 'Santiago@@@' 'Bogotá@@@' 'Chicago###'\n",
      " '  New York###   ' 'Sevilla###' 'Guadalajara###' 'Barranquilla***'\n",
      " 'Valencia@@@' '  cARTAGENA   ' 'Bucaramanga***' 'Medellín###'\n",
      " '  Guadalajara@@@   ' 'Ciudad de México***' 'sANTIAGO***' 'Chicago@@@'\n",
      " 'Cartagena###' 'Bogotá******' 'cONCEPCIÓN@@@' '  Buenos Aires###   '\n",
      " 'Santiago******' 'pUEBLA@@@']\n",
      "\n",
      "'Fecha' ['2025-11-28' '2025-11-29' '2025-12-07' '2025-12-01' '2025-11-18'\n",
      " '2025-11-20' '2025-11-15' '2025-11-30' '2025-11-26' '2025-11-09'\n",
      " '2025-11-16' '2025-11-12' '2025-11-14' '2025-12-04' '2025-11-22'\n",
      " '2025-12-02' '2025-11-24' '2025-11-08' '2025-11-11' '  2025-11-10   '\n",
      " '2025-11-19' '2025-11-23' '2025-11-13' '2025-11-17' '2025-12-05'\n",
      " '2025-12-03' '2025-11-10' '2025-11-21' '2025-11-25' '2025-12-06'\n",
      " '2025-11-27' nan '2025-11-27###' '2025-11-21@@@' '2025-11-24@@@'\n",
      " '  2025-11-27   ' '  2025-11-29   ' '2025-12-05***' '  2025-11-12   '\n",
      " '  2025-12-02   ' '2025-11-10@@@' '  2025-11-16   ' '2025-11-29###'\n",
      " '2025-11-13***' '  2025-11-22   ' '2025-11-17@@@' '2025-12-03@@@'\n",
      " '  2025-11-19   ' '2025-11-19@@@' '2025-11-29@@@' '  2025-12-04   '\n",
      " '  2025-11-25   ' '  2025-11-23   ' '2025-11-16###' '2025-12-07***'\n",
      " '  2025-11-21   ' '  2025-12-07   ' '  2025-11-15   ' '2025-11-17***'\n",
      " '  2025-11-11   ' '2025-11-21###' '  2025-12-05   ' '  2025-12-06   '\n",
      " '  2025-11-09   ' '2025-11-13@@@' '  2025-11-14   ' '  2025-11-30   '\n",
      " '  2025-12-01   ' '2025-11-12@@@' '  2025-11-17   ' '  2025-12-03   '\n",
      " '2025-11-20***' '  2025-11-08   ' '2025-12-06***' '2025-11-12###'\n",
      " '2025-11-14@@@' '  2025-11-28   ' '2025-12-05@@@' '  2025-11-13   '\n",
      " '  2025-11-20   ' '2025-11-09***' '2025-11-17###' '2025-12-02***'\n",
      " '2025-12-04@@@' '2025-11-16***' '2025-11-09@@@' '2025-11-23###'\n",
      " '2025-11-29***' '2025-11-19###' '2025-11-15###' '2025-11-11@@@'\n",
      " '  2025-11-18   ' '2025-11-21***' '2025-11-08@@@' '2025-11-24***'\n",
      " '2025-11-19***' '2025-12-06@@@' '2025-11-25###' '2025-11-30***'\n",
      " '2025-12-07###' '2025-12-01###' '2025-11-08###' '2025-11-08***'\n",
      " '  2025-11-26   ' '2025-12-02@@@' '2025-11-28@@@' '2025-11-22###'\n",
      " '2025-11-26###' '2025-11-13###' '2025-11-27***' '2025-11-30###'\n",
      " '2025-11-15***' '2025-12-04###' '2025-11-25***' '2025-12-05###'\n",
      " '2025-11-27@@@' '2025-12-06###' '2025-11-30@@@' '2025-12-01***'\n",
      " '2025-12-03***' '2025-11-11###' '2025-12-04***' '2025-11-09###'\n",
      " '2025-11-28###' '2025-11-10***' '2025-11-25@@@' '2025-11-18***'\n",
      " '2025-11-15@@@' '2025-11-14###' '2025-11-23***' '2025-11-11***'\n",
      " '  2025-11-24   ' '2025-12-07@@@' '2025-12-02###' '2025-11-16@@@'\n",
      " '2025-12-03###' '2025-11-14***' '2025-11-24###' '2025-11-18###'\n",
      " '2025-11-20###' '2025-11-22***' '2025-11-10###' '2025-11-26@@@'\n",
      " '2025-11-12***' '2025-11-20@@@' '2025-12-01@@@' '2025-11-23@@@'\n",
      " '2025-11-18@@@' '2025-11-28***' '2025-11-26***' '2025-11-22@@@'\n",
      " '  2025-11-25   ###' '  2025-12-05   ###' '  2025-12-03***   ']\n",
      "\n",
      "'Producto' ['Leche' 'Café' 'Té' 'Chocolate' 'Queso' 'Pan' 'Arepa' 'Yogurt' nan\n",
      " 'yOGURT' 'lECHE' 'Yogurt###' 'pAN' '  Té   ' 'Queso###' '  Pan   '\n",
      " 'Chocolate***' 'qUESO' 'aREPA' 'Pan@@@' 'Pan***' '  Queso   ' 'Leche@@@'\n",
      " '  Leche   ' 'Yogurt***' 'Queso@@@' 'tÉ' 'cHOCOLATE' 'Té###' '  Arepa   '\n",
      " 'Pan###' 'Arepa###' 'Café***' 'Café###' 'Té***' 'Queso***' 'Arepa@@@'\n",
      " 'Té@@@' 'Leche***' '  Yogurt   ' 'Leche###' 'Arepa***' '  Café   '\n",
      " '  Chocolate   ' 'cAFÉ' 'Café@@@' 'Chocolate@@@' 'Chocolate###'\n",
      " 'Yogurt@@@' '    Queso      ' '  tÉ   ' 'yOGURT***' 'aREPA@@@']\n",
      "\n",
      "'Tipo_Producto' ['Alimento_Percedero' 'Hogar' 'Snack' 'Lácteo' 'Bebida' nan 'Abarrotes'\n",
      " 'Abarrotes@@@' '  Hogar   ' 'sNACK' 'Lácteo***' 'Alimento_Percedero@@@'\n",
      " 'Lácteo@@@' '  Bebida   ' 'aBARROTES' 'aLIMENTO_pERCEDERO' 'Abarrotes###'\n",
      " 'Lácteo###' 'bEBIDA' 'lÁCTEO' '  Snack   ' '  Abarrotes   ' 'Hogar###'\n",
      " 'Bebida###' 'Abarrotes***' 'Alimento_Percedero***' 'Hogar***' 'hOGAR'\n",
      " '  Alimento_Percedero   ' 'Snack@@@' '  Lácteo   ' 'Snack###' 'Hogar@@@'\n",
      " 'Bebida***' 'Bebida@@@' 'Alimento_Percedero###' 'Snack***' '  hOGAR   '\n",
      " 'Bebida@@@@@@' '  aLIMENTO_pERCEDERO   ' '  Lácteo###   '\n",
      " '    Hogar      ']\n",
      "\n",
      "'Cantidad' ['2.0' '5.0' '1.0' '7.0' '4.0' '6.0' '10.0' '8.0' '9.0' '3.0' '500.0' nan\n",
      " '125.0' '100.0' '???' '70.0' '90.0' '50.0' '75.0' '200.0' '225.0' '250.0'\n",
      " '20.0' '300.0' '80.0' '25.0' '150.0' '30.0' '450.0' '400.0' '350.0'\n",
      " '175.0' '60.0' '40.0' '5000.0' '22500.0' '20000.0' '1250.0' '3000.0'\n",
      " '25000.0' '2500.0' '2000.0' '750.0' '700.0']\n",
      "\n",
      "'Precio_Unitario' ['1587.0' nan '3882.0' ... '230000.0' '173200.0' '73625.0']\n",
      "\n",
      "'Tipo_Venta' ['Online' 'Call_Center' 'Tienda_Física' 'Distribuidor' nan '  Online   '\n",
      " '  Call_Center   ' 'cALL_cENTER' 'dISTRIBUIDOR' 'Online###'\n",
      " '  Distribuidor   ' 'Tienda_Física###' '  Tienda_Física   ' 'Online@@@'\n",
      " 'Call_Center###' 'oNLINE' 'Online***' 'Tienda_Física@@@'\n",
      " 'Distribuidor@@@' 'tIENDA_fÍSICA' 'Call_Center@@@' 'Distribuidor***'\n",
      " 'Call_Center***' 'Tienda_Física***' 'Distribuidor###' 'dISTRIBUIDOR###'\n",
      " 'cALL_cENTER@@@' 'Tienda_Física@@@@@@']\n",
      "\n",
      "'Tipo_Cliente' ['Minorista' 'Mayorista' 'Corporativo' 'Gobierno' nan 'Mayorista@@@'\n",
      " 'mINORISTA' '  Corporativo   ' '  Gobierno   ' 'Gobierno###'\n",
      " 'Gobierno***' 'Corporativo###' '  Mayorista   ' 'cORPORATIVO' 'mAYORISTA'\n",
      " 'Corporativo@@@' '  Minorista   ' 'Minorista@@@' 'Mayorista***'\n",
      " 'gOBIERNO' 'Corporativo***' 'Minorista###' 'Mayorista###' 'Gobierno@@@'\n",
      " 'Minorista***' '    Corporativo      ' 'Minorista###***'\n",
      " '  Minorista   @@@' '  mINORISTA   ' '  Gobierno***   ']\n",
      "\n",
      "'Descuento' ['0.2' '0.0' '0.15' '0.05' '0.1' '???' nan]\n",
      "\n",
      "'Costo_Envio' ['0.0' '10000.0' '250000.0' nan '5000.0' '???' '100000.0' '500000.0'\n",
      " '125000.0' '50000.0' '2500000.0' '3125000.0' '1000000.0' '6250000.0'\n",
      " '12500000.0']\n",
      "\n",
      "'Total' ['2539.0' '20412.0' '3882.0' ... '38991.0' '102770.0' '1065100.0']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in df.columns:\n",
    "    print(f\"'{row}' {df[row].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ecdf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data per columnCiudad             4551\n",
      "Fecha              4620\n",
      "Producto           4605\n",
      "Tipo_Producto      4626\n",
      "Cantidad           4530\n",
      "Precio_Unitario    4455\n",
      "Tipo_Venta         4523\n",
      "Tipo_Cliente       4421\n",
      "Descuento          4503\n",
      "Costo_Envio        4469\n",
      "Total              4528\n",
      "dtype: int64\n",
      "Total data missing 49831\n"
     ]
    }
   ],
   "source": [
    "mis_columns = df.isna().sum()\n",
    "mis_total = df.isna().sum().sum()\n",
    "\n",
    "print(f\"Missing data per column{ mis_columns}\\nTotal data missing {mis_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1809de2",
   "metadata": {},
   "source": [
    "#### 2. Cleaning DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71efa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    \n",
    "    # text to lower case\n",
    "    texto = texto.lower().strip()\n",
    "\n",
    "    # delet ascents\n",
    "    texto = ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    # delet special characters only keep letters\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "\n",
    "    # remove double spaces \"  \"\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "\n",
    "    return texto\n",
    "\n",
    "# Normalizing text\n",
    "cols_texto = [\"Ciudad\", \"Producto\", \"Tipo_Producto\", \"Tipo_Venta\", \"Tipo_Cliente\"]\n",
    "for col in cols_texto:\n",
    "    df[col] = df[col].apply(clean_city)\n",
    "\n",
    "# Conversion to numeric\n",
    "df[\"Descuento\"] = pd.to_numeric(df[\"Descuento\"], errors=\"coerce\")\n",
    "df[\"Precio_Unitario\"] = pd.to_numeric(df[\"Precio_Unitario\"], errors=\"coerce\")\n",
    "df[\"Cantidad\"] = pd.to_numeric(df[\"Cantidad\"], errors=\"coerce\")\n",
    "df[\"Costo_Envio\"] = pd.to_numeric(df[\"Costo_Envio\"], errors=\"coerce\")\n",
    "df[\"Total\"] = pd.to_numeric(df[\"Total\"], errors=\"coerce\")\n",
    "\n",
    "# Fill missing\n",
    "df[\"Tipo_Cliente\"] = df[\"Tipo_Cliente\"].replace(\"nan\", np.nan)\n",
    "df[\"Tipo_Cliente\"] = df[\"Tipo_Cliente\"].fillna(\"desconocido\")\n",
    "\n",
    "df[\"Descuento\"] = df[\"Descuento\"].fillna(0)\n",
    "df[\"Precio_Unitario\"] = df[\"Precio_Unitario\"].fillna(0)\n",
    "df[\"Cantidad\"] = df[\"Cantidad\"].fillna(0)\n",
    "df[\"Costo_Envio\"] = df[\"Costo_Envio\"].fillna(0)\n",
    "df[\"Total\"] = df[\"Total\"].fillna(0)\n",
    "\n",
    "# Conversion to date format\n",
    "df[\"Fecha\"] = pd.to_datetime(df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Recalculating \"Total\" column\n",
    "df[\"Total\"] = (df[\"Cantidad\"] * df[\"Precio_Unitario\"]) - (df[\"Descuento\"] * df[\"Cantidad\"] * df[\"Precio_Unitario\"]) + df[\"Costo_Envio\"]\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# 6. Save changes in a new .csv\n",
    "\n",
    "df.to_csv(\"clean_sales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b34038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ciudad' ['antofagasta' 'monterrey' 'valparaiso' 'sevilla' 'cordoba'\n",
      " 'ciudad de mexico' 'houston' 'mendoza' 'barcelona' 'chicago' 'miami'\n",
      " 'pereira' 'cali' 'concepcion' 'rosario' 'madrid' 'arequipa' 'lima'\n",
      " 'santiago' 'bucaramanga' 'tijuana' 'guadalajara' 'cusco' 'new york'\n",
      " 'buenos aires' 'valencia' 'los angeles' 'bogota' 'trujillo' nan 'puebla'\n",
      " 'medellin' 'barranquilla' 'cartagena']\n",
      "\n",
      "'fecha' <DatetimeArray>\n",
      "['2025-11-28 00:00:00', '2025-11-29 00:00:00', '2025-12-07 00:00:00',\n",
      " '2025-12-01 00:00:00', '2025-11-18 00:00:00', '2025-11-20 00:00:00',\n",
      " '2025-11-15 00:00:00', '2025-11-30 00:00:00', '2025-11-26 00:00:00',\n",
      " '2025-11-09 00:00:00', '2025-11-16 00:00:00', '2025-11-12 00:00:00',\n",
      " '2025-11-14 00:00:00', '2025-12-04 00:00:00', '2025-11-22 00:00:00',\n",
      " '2025-12-02 00:00:00', '2025-11-24 00:00:00', '2025-11-08 00:00:00',\n",
      " '2025-11-11 00:00:00',                 'NaT', '2025-11-19 00:00:00',\n",
      " '2025-11-23 00:00:00', '2025-11-13 00:00:00', '2025-11-17 00:00:00',\n",
      " '2025-12-05 00:00:00', '2025-12-03 00:00:00', '2025-11-10 00:00:00',\n",
      " '2025-11-21 00:00:00', '2025-11-25 00:00:00', '2025-12-06 00:00:00',\n",
      " '2025-11-27 00:00:00']\n",
      "Length: 31, dtype: datetime64[ns]\n",
      "\n",
      "'producto' ['leche' 'cafe' 'te' 'chocolate' 'queso' 'pan' 'arepa' 'yogurt' nan]\n",
      "\n",
      "'tipo_producto' ['alimentopercedero' 'hogar' 'snack' 'lacteo' 'bebida' nan 'abarrotes']\n",
      "\n",
      "'cantidad' [2.00e+00 5.00e+00 1.00e+00 7.00e+00 4.00e+00 6.00e+00 1.00e+01 8.00e+00\n",
      " 9.00e+00 3.00e+00 5.00e+02 0.00e+00 1.25e+02 1.00e+02 7.00e+01 9.00e+01\n",
      " 5.00e+01 7.50e+01 2.00e+02 2.25e+02 2.50e+02 2.00e+01 3.00e+02 8.00e+01\n",
      " 2.50e+01 1.50e+02 3.00e+01 4.50e+02 4.00e+02 3.50e+02 1.75e+02 6.00e+01\n",
      " 4.00e+01 5.00e+03 2.25e+04 2.00e+04 1.25e+03 3.00e+03 2.50e+04 2.50e+03\n",
      " 2.00e+03 7.50e+02 7.00e+02]\n",
      "\n",
      "'precio_unitario' [  1587.      0.   3882. ... 230000. 173200.  73625.]\n",
      "\n",
      "'tipo_venta' ['online' 'callcenter' 'tiendafisica' 'distribuidor' nan]\n",
      "\n",
      "'tipo_cliente' ['minorista' 'mayorista' 'corporativo' 'gobierno' 'desconocido'\n",
      " 'minorista ']\n",
      "\n",
      "'descuento' [0.2  0.   0.15 0.05 0.1 ]\n",
      "\n",
      "'costo_envio' [0.000e+00 1.000e+04 2.500e+05 5.000e+03 1.000e+05 5.000e+05 1.250e+05\n",
      " 5.000e+04 2.500e+06 3.125e+06 1.000e+06 6.250e+06 1.250e+07]\n",
      "\n",
      "'total' [ 2539.2  10000.    3882.   ... 32635.   29112.75 33573.9 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in df.columns:\n",
    "    print(f\"'{row}' {df[row].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f5e32",
   "metadata": {},
   "source": [
    "#### 3. PostgreDB connection configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ac03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Succesfull!!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "#Variables de entorno\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "\n",
    "URL = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(URL)\n",
    "\n",
    "conn = engine.connect()\n",
    "try:\n",
    "    print(\"Connection Succesfull!!\" if conn else \"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error al conectar la base datos en\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b17de",
   "metadata": {},
   "source": [
    "#### 4. Setting schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063c225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: 'Ciudad' Is empty.\n",
      "Table: 'Tipo_producto' Is empty.\n",
      "Table: 'Producto' Is empty.\n",
      "Table: 'Tipo_venta' Is empty.\n",
      "Table: 'Tipo_cliente' Is empty.\n",
      "Table: 'Factura_ventas' Is empty.\n"
     ]
    }
   ],
   "source": [
    "# Setting schema\n",
    "\n",
    "set_schema = conn.execute(text(\"set search_path to riwi_ventas;\"))\n",
    "\n",
    "# Array to verify the status of each one table\n",
    "tables = [\"ciudad\", \"tipo_producto\", \"producto\", \"tipo_venta\", \"tipo_cliente\", \"factura_ventas\"]\n",
    "\n",
    "# This loop verfy the status\n",
    "for t in range(len(tables)):\n",
    "    verify = pd.read_sql((f\"select * from {tables[t]};\"),conn)\n",
    "    if not verify.empty:\n",
    "        print(f\"Talbe: '{tables[t].capitalize()}' Is Filled.\")\n",
    "    if verify.empty:\n",
    "        print(f\"Table: '{tables[t].capitalize()}' Is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bd23f",
   "metadata": {},
   "source": [
    "#### 4.1 Uploading data to PostgresDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f4fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV creation sucessfull 'tables_csv'\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = pd.read_csv(\"clean_sales.csv\")\n",
    "\n",
    "#Obtain the unique cities and generate the ID for Cities\n",
    "ciudad_df = (\n",
    "    cleaned_df[\"ciudad\"].drop_duplicates()\n",
    "                .reset_index(drop=True)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"ciudad_id\", \"ciudad\": \"nombre_ciudad\"})\n",
    "                \n",
    ")\n",
    "ciudad_df[\"ciudad_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(ciudad_df, left_on=\"ciudad\", right_on=\"nombre_ciudad\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for Products\n",
    "\n",
    "tipo_producto_df = (\n",
    "    cleaned_df[\"tipo_producto\"].drop_duplicates()\n",
    "                       .reset_index(drop=True)\n",
    "                       .reset_index()\n",
    "                       .rename(columns={\"index\": \"tipo_producto_id\"})\n",
    ")\n",
    "tipo_producto_df[\"tipo_producto_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_producto_df, on=\"tipo_producto\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type products\n",
    "\n",
    "cleaned_df = cleaned_df.rename(columns={\"producto\":\"nombre_producto\"}) #Rename to prevent conflicts with DB\n",
    "\n",
    "producto_df = (\n",
    "    cleaned_df[[\"nombre_producto\", \"tipo_producto_id\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"producto_id\", \"producto\":\"nombre_producto\"})\n",
    ")\n",
    "producto_df[\"producto_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(producto_df, on=[\"nombre_producto\", \"tipo_producto_id\"], how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type sale\n",
    "\n",
    "cleaned_df = cleaned_df.rename(columns={\"tipo_de_venta\":\"tipo_venta\"}) #Rename to prevent conflicts with DB\n",
    "\n",
    "tipo_venta_df = (\n",
    "    cleaned_df[\"tipo_venta\"].drop_duplicates()\n",
    "                       .reset_index(drop=True)\n",
    "                       .reset_index()\n",
    "                       .rename(columns={\"index\": \"tipo_venta_id\"})\n",
    ")\n",
    "tipo_venta_df[\"tipo_venta_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_venta_df, on=\"tipo_venta\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type customer\n",
    "tipo_cliente_df = (\n",
    "    cleaned_df[\"tipo_cliente\"].drop_duplicates()\n",
    "                      .reset_index(drop=True)\n",
    "                      .reset_index()\n",
    "                      .rename(columns={\"index\": \"tipo_cliente_id\"})\n",
    ")\n",
    "tipo_cliente_df[\"tipo_cliente_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_cliente_df, on=\"tipo_cliente\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "#Create the table for sales\n",
    "factura_df =cleaned_df[[\"fecha\", \"ciudad_id\", \"producto_id\", \"tipo_venta_id\", \"tipo_cliente_id\", \"cantidad\", \"precio_unitario\", \"descuento\", \"costo_envio\", \"total\"]].copy()\n",
    "\n",
    "\n",
    "#export tables to external folder\n",
    "os.makedirs(\"tables_csv\", exist_ok=True)\n",
    "\n",
    "#Save the files into tables_csv\n",
    "ciudad_df.to_csv(\"tables_csv/ciudad.csv\", index=False)\n",
    "tipo_producto_df.to_csv(\"tables_csv/tipo_producto.csv\", index=False)\n",
    "producto_df.to_csv(\"tables_csv/producto.csv\", index=False)\n",
    "tipo_venta_df.to_csv(\"tables_csv/tipo_venta.csv\", index=False)\n",
    "tipo_cliente_df.to_csv(\"tables_csv/tipo_cliente.csv\", index=False)\n",
    "factura_df.to_csv(\"tables_csv/factura_ventas.csv\", index=False)\n",
    "\n",
    "print(\"CSV creation sucessfull 'tables_csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973f3f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Insert into table: ciudad\n",
      "Chunk 1 inserting... (34 rows)\n",
      "Table ciudad was inserted sucess!!\n",
      "\n",
      "Insert into table: tipo_producto\n",
      "Chunk 1 inserting... (7 rows)\n",
      "Table tipo_producto was inserted sucess!!\n",
      "\n",
      "Insert into table: producto\n",
      "Chunk 1 inserting... (63 rows)\n",
      "Table producto was inserted sucess!!\n",
      "\n",
      "Insert into table: tipo_venta\n",
      "Chunk 1 inserting... (5 rows)\n",
      "Table tipo_venta was inserted sucess!!\n",
      "\n",
      "Insert into table: tipo_cliente\n",
      "Chunk 1 inserting... (6 rows)\n",
      "Table tipo_cliente was inserted sucess!!\n",
      "\n",
      "Insert into table: factura_ventas\n",
      "Chunk 1 inserting... (100000 rows)\n",
      "Chunk 2 inserting... (100000 rows)\n",
      "Chunk 3 inserting... (100000 rows)\n",
      "Chunk 4 inserting... (100000 rows)\n",
      "Chunk 5 inserting... (100000 rows)\n",
      "Chunk 6 inserting... (100000 rows)\n",
      "Chunk 7 inserting... (100000 rows)\n",
      "Chunk 8 inserting... (100000 rows)\n",
      "Chunk 9 inserting... (100000 rows)\n",
      "Chunk 10 inserting... (100000 rows)\n",
      "Table factura_ventas was inserted sucess!!\n",
      "Data inserted sucessfull!!\n"
     ]
    }
   ],
   "source": [
    "#This order respect the cardinality and prevent issues\n",
    "order_tables = [\n",
    "    \"ciudad\",\n",
    "    \"tipo_producto\",\n",
    "    \"producto\",\n",
    "    \"tipo_venta\",\n",
    "    \"tipo_cliente\",\n",
    "    \"factura_ventas\"\n",
    "]\n",
    "\n",
    "SEEDERS_DIR = \"tables_csv\"\n",
    "\n",
    "CHUNK_SIZE = 100000  \n",
    "\n",
    "with engine.connect() as conn:\n",
    "    \n",
    "    conn.execute(text(\"SET search_path TO riwi_ventas;\"))\n",
    "\n",
    "    for table in order_tables:\n",
    "        csv_path = os.path.join(SEEDERS_DIR, f\"{table}.csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"doesn't exist: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nInsert into table: {table}\")\n",
    "\n",
    "        # Read by chunk\n",
    "        chunk_iter = pd.read_csv(csv_path, chunksize=CHUNK_SIZE)\n",
    "\n",
    "        for i, chunk_df in enumerate(chunk_iter):\n",
    "            print(f\"Chunk {i+1} inserting... ({len(chunk_df)} rows)\")\n",
    "\n",
    "            # Replace NaN per None or Null for SQL\n",
    "            chunk_df = chunk_df.where(pd.notnull(chunk_df), None)\n",
    "\n",
    "            cols = chunk_df.columns.tolist()\n",
    "            colnames = \", \".join(cols)\n",
    "            placeholders = \", \".join([f\":{c}\" for c in cols])\n",
    "\n",
    "            query = text(f\"\"\"\n",
    "                INSERT INTO {table} ({colnames})\n",
    "                VALUES ({placeholders});\n",
    "            \"\"\")\n",
    "\n",
    "            # Ejecutar todas las filas del chunk\n",
    "            conn.execute(query, chunk_df.to_dict(orient=\"records\"))\n",
    "\n",
    "        print(f\"Table {table} was inserted sucess!!\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "print(\"Data inserted sucessfull!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9bf83",
   "metadata": {},
   "source": [
    "#### 5. Visualization of Data in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdef9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "| Tablas encontradas: 6 |\n",
      "-------------------------\n",
      "1. tipo_producto\n",
      "2. producto\n",
      "3. ciudad\n",
      "4. factura_ventas\n",
      "5. tipo_venta\n",
      "6. tipo_cliente\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Table: tipo_producto exist and is filled\n",
      "Table: producto exist and is filled\n",
      "Table: ciudad exist and is filled\n",
      "Table: factura_ventas exist and is filled\n",
      "Table: tipo_venta exist and is filled\n",
      "Table: tipo_cliente exist and is filled\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    # Setting schema\n",
    "    conn.execute(text(\"SET search_path TO riwi_ventas\"))\n",
    "    \n",
    "    # Get table list\n",
    "    table_list = pd.read_sql(text(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'riwi_ventas';\"),conn)\n",
    "    \n",
    "    msg = f\"| Tablas encontradas: {len(table_list)} |\"\n",
    "    \n",
    "    print(\"-\"*len(msg))\n",
    "    print(msg)\n",
    "    print(\"-\"*len(msg))\n",
    "\n",
    "    idx = 0\n",
    "    for table in table_list[\"table_name\"]:\n",
    "        idx+=1\n",
    "        print(f\"{idx}. {table}\")\n",
    "        \n",
    "    print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    # List existing and filled tables.\n",
    "    for table in table_list[\"table_name\"]:\n",
    "        verfy = pd.read_sql(text(f\"select * from {table} LIMIT 1000;\"),conn)\n",
    "        if not verfy.empty:\n",
    "            print(f\"Table: {table} exist and is filled\")\n",
    "        else:\n",
    "            print(f\"Table: | {table} | doesn't exist or is empty please check in the Data base.\")\n",
    "\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
