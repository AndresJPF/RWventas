Aquí tienes **un README completo, profesional y bien estructurado en inglés**, listo para usar en tu repositorio **RWventas**.
Solo copia y pega en tu archivo `README.md`.
(Si quieres que también genere las *capturas con títulos y descripciones*, envíame las imágenes y lo agrego.)

---

# RWventas – Sales Data Cleaning, Modeling & Dashboard Project

## Project Overview

This project processes, cleans, and normalizes a raw sales dataset (`RWventas.csv`), loads it into a PostgreSQL database following a normalized relational model, and generates multiple visualizations and business insights.

The repository includes:

* Data cleaning using **Pandas**
* Automated ID generation for dimension tables
* Export of normalized tables into CSV files
* PostgreSQL insertion using **SQLAlchemy**
* Dashboard creation in Python
* Insights & recommendations

---

# 1. Database Connection

The project uses **SQLAlchemy** and environment variables stored in a `.env` file.

### .env Structure

```
DB_USER=your_user
DB_PASSWORD=your_password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=riwi_ventas
```

### Original CSV
* https://drive.google.com/drive/folders/1QLof4b9BV_74F90kgTKHcTCnxDs2iRIG?usp=sharing

### Connection Code

```python
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
import os

load_dotenv(override=True)

DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")
DB_NAME = os.getenv("DB_NAME")

URL = f"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
engine = create_engine(URL)

conn = engine.connect()
print("Connection Successful!")
```

---

# 2. Data Cleaning Steps

The raw file `RWventas.csv` is cleaned using the following transformations:

### Normalize text fields

* Convert to lowercase
* Remove accents
* Remove special characters
* Remove multiple spaces

### Convert numeric fields

```python
df["Descuento"] = pd.to_numeric(df["Descuento"], errors="coerce")
df["Precio_Unitario"] = pd.to_numeric(df["Precio_Unitario"], errors="coerce")
df["Cantidad"] = pd.to_numeric(df["Cantidad"], errors="coerce")
```

### Handle missing values

* Replace `"nan"` in text fields
* Fill numeric columns with `0`

### Convert to datetime

```python
df["Fecha"] = pd.to_datetime(df["Fecha"], errors="coerce")
```

### Recalculate totals

```
Total = Cantidad × Precio_Unitario − (Descuento × Cantidad × Precio_Unitario) + Costo_Envio
```

### Save cleaned dataset

```
clean_sales.csv
```

---

# 3. Creating the Normalized Database Structure

Your project follows this relational diagram:

*(Insert your schema image here)*

### Tables Created:

| Table            | Description      |
| ---------------- | ---------------- |
| `ciudad`         | City dimension   |
| `tipo_producto`  | Product category |
| `producto`       | Products         |
| `tipo_venta`     | Sale type        |
| `tipo_cliente`   | Customer type    |
| `factura_ventas` | Fact table       |

Each table receives autogenerated IDs using Pandas, then everything is exported into the folder:

**tables_csv/**

---

# 4. Inserting Data into PostgreSQL

The script loads CSVs in the correct order to respect foreign keys:

```python
order_tables = [
    "ciudad",
    "tipo_producto",
    "producto",
    "tipo_venta",
    "tipo_cliente",
    "factura_ventas"
]
```

Each table is inserted in chunks using SQLAlchemy + `INSERT INTO`.

A validation step confirms:

* Tables exist
* Tables are filled

Output example:

```
Table 'producto' is filled.
Table 'factura_ventas' is filled.
```

---

# 5. Visualizations – Step-by-Step

Below is the recommended process to create each dashboard.

---

## **Visualization 1: Sales by City**

```python
df.groupby("ciudad")["total"].sum().sort_values().plot(kind="bar")
plt.title("Total Sales by City")
plt.show()
```

---

## **Visualization 2: Top Products**

```python
df.groupby("nombre_producto")["total"].sum().sort_values(ascending=False).head(10).plot(kind="bar")
plt.title("Top-Selling Products")
plt.show()
```

---

## **Visualization 3: Revenue by Sale Type**

```python
df.groupby("tipo_venta")["total"].sum().plot(kind="pie")
plt.title("Revenue Distribution by Sale Type")
plt.show()
```

---

## **Visualization 4: Monthly Sales Trend**

```python
df.groupby(df["fecha"].dt.to_period("M"))["total"].sum().plot()
plt.title("Monthly Sales Trend")
plt.show()
```

---

# 6. Insights & Recommendations

### Key Insights

* **City X** generates the highest revenue — focus marketing there.
* **Product category Y** is the most profitable.
* **Online sales** outperform physical sales (if shown in data).
* There are noticeable **seasonal trends** in months A–B.
* Some cities have **high shipping cost impact**, reducing net revenue.

### Recommendations

1. Increase stock for the top 10 products.
2. Improve logistics in cities with high shipping costs.
3. Launch targeted promotions in low-performing regions.
4. Boost online marketing campaigns — high ROI.
5. Review discount policies that significantly reduce total revenue.

---

# Repository Structure

```
RWventas/
│── RWventas.csv
│── clean_sales.csv
│── assets/
│── images/
│   │── Dashboar_Map.png
│   │── Main_Dashboard.png
│   │── schema_image.png
│── reports/
│   │── Report_Sales.pdf
│   │── Sales_Dashboards.pbix
│── tables_csv/
│   ├── ciudad.csv
│   ├── tipo_producto.csv
│   ├── producto.csv
│   ├── tipo_venta.csv
│   ├── tipo_cliente.csv
│   └── factura_ventas.csv
│── scripts/
│── README.md
```

---

# Final Notes

This project demonstrates:

* Full ETL pipeline (Extract → Transform → Load)
* Relational modeling
* Data storytelling through dashboards
* Clean and automated database insertion

